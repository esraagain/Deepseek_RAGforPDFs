{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert research assistant. Use the provided context to answer the query. \n",
    "If unsure, state that you don't know. Be concise and factual (max 3 sentences).\n",
    "\n",
    "Query: {user_query} \n",
    "Context: {document_context} \n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Configuration\n",
    "PDF_STORAGE_PATH = 'E:/projects/python/genai/RAG/deepseek/document_store/'  # Ensure trailing slash\n",
    "EMBEDDING_MODEL = OllamaEmbeddings(model=\"deepseek-r1:1.5b\")\n",
    "DOCUMENT_VECTOR_DB = InMemoryVectorStore(EMBEDDING_MODEL)\n",
    "LANGUAGE_MODEL = OllamaLLM(model=\"deepseek-r1:1.5b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the uploaded file to the specified path\n",
    "def save_uploaded_file(file_path):\n",
    "    import os\n",
    "    os.makedirs(PDF_STORAGE_PATH, exist_ok=True)\n",
    "    return file_path\n",
    "\n",
    "# Load PDF documents\n",
    "def load_pdf_documents(file_path):\n",
    "    document_loader = PDFPlumberLoader(file_path)\n",
    "    return document_loader.load()\n",
    "\n",
    "# Split documents into chunks\n",
    "def chunk_documents(raw_documents):\n",
    "    text_processor = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        add_start_index=True\n",
    "    )\n",
    "    return text_processor.split_documents(raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index document chunks into the vector store\n",
    "def index_documents(document_chunks):\n",
    "    DOCUMENT_VECTOR_DB.add_documents(document_chunks)\n",
    "\n",
    "# Find related documents based on a query\n",
    "def find_related_documents(query):\n",
    "    return DOCUMENT_VECTOR_DB.similarity_search(query)\n",
    "\n",
    "# Generate an answer using the language model\n",
    "def generate_answer(user_query, context_documents):\n",
    "    context_text = \"\\n\\n\".join([doc.page_content for doc in context_documents])\n",
    "    conversation_prompt = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    response_chain = conversation_prompt | LANGUAGE_MODEL\n",
    "    return response_chain.invoke({\"user_query\": user_query, \"document_context\": context_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Document processed successfully! Ask your questions below.\n",
      "Answer: <think>\n",
      "Okay, so I'm trying to figure out the role of the encoder in the Transformer model as explained in this context. From what I remember, Transformers are a type of neural network architecture developed by Google. They're known for their efficiency and effectiveness in various tasks like machine translation.\n",
      "\n",
      "In the provided context, it seems that the Transformer has an encoder-decoder structure throughout the sections. The encoder is described as mapping an input sequence of symbol representations to a sequence of continuous representations z. This sounds familiar. I think the encoder processes the input data and transforms it into a form that can be used later by the decoder.\n",
      "\n",
      "The decoder then generates an output sequence based on this transformed data. Each step in the model is auto-regressive, meaning each generated element uses the previously generated ones as additional inputs. That makes sense because in many sequential tasks like text generation or music synthesis, generating one part requires knowing the preceding parts.\n",
      "\n",
      "Looking at Table3, there are variations of the Transformer architecture with different configurations and metrics like perplexity, which is a measure of how well a model predicts the next word. The perplexity metrics across different models indicate their performance on the English-to-German translation task using beam search and without checkpoint averaging. This suggests that the encoder's role in transforming the input into this continuous representation z is crucial for capturing the necessary information for accurate translation.\n",
      "\n",
      "I also notice that all the mentioned architectures are based on the base model, which implies that variations include different components or parameters that affect the output but not the core encoder function. So, regardless of the specific architecture (like different layer depths or attention mechanisms), the encoder's main job is to convert input into a continuous vector representation.\n",
      "\n",
      "To summarize, the encoder in the Transformer model transforms an input sequence into continuous representations, enabling the decoder to generate subsequent outputs step by step. This setup allows the model to handle complex sequential data effectively and perform tasks like translation with high accuracy.\n",
      "</think>\n",
      "\n",
      "The encoder in the Transformer model converts an input sequence into a continuous representation z through auto-regressive processing. It generates output based on this transformed data, allowing the model to sequentially build sequences as needed for tasks like text generation or translation.\n",
      "\n",
      "Answer: The encoder transforms input into a continuous vector, enabling sequential output generation where each step uses previously generated elements.\n"
     ]
    }
   ],
   "source": [
    "# Main program\n",
    "if __name__ == \"__main__\":\n",
    "    # Prompt user to upload a PDF file\n",
    "    pdf_path = input(\"Enter the path to your PDF file: \")\n",
    "    saved_path = save_uploaded_file(pdf_path)\n",
    "\n",
    "    # Load, process, and index the PDF\n",
    "    raw_docs = load_pdf_documents(saved_path)\n",
    "    processed_chunks = chunk_documents(raw_docs)\n",
    "    index_documents(processed_chunks)\n",
    "\n",
    "    print(\"✅ Document processed successfully! Ask your questions below.\")\n",
    "\n",
    "    # Interactive loop for user queries\n",
    "    while True:\n",
    "        user_input = input(\"Enter your question about the document: \")\n",
    "        print(\"Responding to\", user_input)\n",
    "        if not user_input.strip():  # Exit if the input is empty\n",
    "            break\n",
    "\n",
    "        # Find related documents and generate an answer\n",
    "        related_docs = find_related_documents(user_input)\n",
    "        answer = generate_answer(user_input, related_docs)\n",
    "        print(\"Answer:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
